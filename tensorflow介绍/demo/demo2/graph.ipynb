{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ljw_h\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_variable(shape):\n",
    "    weights = tf.get_variable(\"weights\", shape, initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor):\n",
    "    with tf.variable_scope('layer1'):\n",
    "        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE])\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer = tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "    \n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE])\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer = tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "    \n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.005\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "TRAINING_STEPS = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    y = inference(x)\n",
    "    train_size = mnist[0][0].shape[0]\n",
    "    mnist_x = np.reshape(mnist[0][0],[-1,784])\n",
    "    mnist_y = np.eye(10)[mnist[0][1]]\n",
    "    \n",
    "    test_feed = {x: np.reshape(mnist[1][0],[-1,784]),y_: np.eye(10)[mnist[1][1]]}\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step,train_size / BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "    \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist_x[i*BATCH_SIZE%train_size:(i+1)*BATCH_SIZE%train_size], mnist_y[i*BATCH_SIZE%train_size:(i+1)*BATCH_SIZE%train_size]\n",
    "            _, loss_value, step = sess.run([train_step, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "                print(\"After {} training steps, loss on training batch is {},test accuracy is {}\".format(step, loss_value, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training steps, loss on training batch is 410.2440490722656,test accuracy is 0.19830000400543213\n",
      "After 101 training steps, loss on training batch is 1.235223412513733,test accuracy is 0.6680999994277954\n",
      "After 201 training steps, loss on training batch is 1.5397768020629883,test accuracy is 0.7210000157356262\n",
      "After 301 training steps, loss on training batch is 1.7024928331375122,test accuracy is 0.7469000220298767\n",
      "After 401 training steps, loss on training batch is 1.144922137260437,test accuracy is 0.7886000275611877\n",
      "After 501 training steps, loss on training batch is 0.924815833568573,test accuracy is 0.8083999752998352\n",
      "After 601 training steps, loss on training batch is 0.49199581146240234,test accuracy is 0.8385000228881836\n",
      "After 701 training steps, loss on training batch is 0.5883651375770569,test accuracy is 0.8492000102996826\n",
      "After 801 training steps, loss on training batch is 0.5868187546730042,test accuracy is 0.8640000224113464\n",
      "After 901 training steps, loss on training batch is 0.8438820838928223,test accuracy is 0.8549000024795532\n",
      "After 1001 training steps, loss on training batch is 0.5292694568634033,test accuracy is 0.8531000018119812\n",
      "After 1101 training steps, loss on training batch is 0.5080961585044861,test accuracy is 0.8697999715805054\n",
      "After 1201 training steps, loss on training batch is 0.2943611443042755,test accuracy is 0.8806999921798706\n",
      "After 1301 training steps, loss on training batch is 0.5020993947982788,test accuracy is 0.8781999945640564\n",
      "After 1401 training steps, loss on training batch is 0.5223265290260315,test accuracy is 0.8881000280380249\n",
      "After 1501 training steps, loss on training batch is 0.5564661026000977,test accuracy is 0.8756999969482422\n",
      "After 1601 training steps, loss on training batch is 0.394726037979126,test accuracy is 0.8765000104904175\n",
      "After 1701 training steps, loss on training batch is 0.37010815739631653,test accuracy is 0.8884000182151794\n",
      "After 1801 training steps, loss on training batch is 0.22009620070457458,test accuracy is 0.8964999914169312\n",
      "After 1901 training steps, loss on training batch is 0.4334932565689087,test accuracy is 0.895799994468689\n",
      "After 2001 training steps, loss on training batch is 0.46543434262275696,test accuracy is 0.9032999873161316\n",
      "After 2101 training steps, loss on training batch is 0.46689194440841675,test accuracy is 0.8949000239372253\n",
      "After 2201 training steps, loss on training batch is 0.336661159992218,test accuracy is 0.8931999802589417\n",
      "After 2301 training steps, loss on training batch is 0.23872511088848114,test accuracy is 0.900600016117096\n",
      "After 2401 training steps, loss on training batch is 0.1822640299797058,test accuracy is 0.9035999774932861\n",
      "After 2501 training steps, loss on training batch is 0.42990559339523315,test accuracy is 0.9056000113487244\n",
      "After 2601 training steps, loss on training batch is 0.41543856263160706,test accuracy is 0.9052000045776367\n",
      "After 2701 training steps, loss on training batch is 0.43822169303894043,test accuracy is 0.9045000076293945\n",
      "After 2801 training steps, loss on training batch is 0.3570544421672821,test accuracy is 0.9017000198364258\n",
      "After 2901 training steps, loss on training batch is 0.23007778823375702,test accuracy is 0.90829998254776\n",
      "After 3001 training steps, loss on training batch is 0.18657246232032776,test accuracy is 0.9117000102996826\n",
      "After 3101 training steps, loss on training batch is 0.4064459204673767,test accuracy is 0.9125999808311462\n",
      "After 3201 training steps, loss on training batch is 0.42872241139411926,test accuracy is 0.9093999862670898\n",
      "After 3301 training steps, loss on training batch is 0.3402199447154999,test accuracy is 0.9143000245094299\n",
      "After 3401 training steps, loss on training batch is 0.29840391874313354,test accuracy is 0.9088000059127808\n",
      "After 3501 training steps, loss on training batch is 0.18289877474308014,test accuracy is 0.913100004196167\n",
      "After 3601 training steps, loss on training batch is 0.17382793128490448,test accuracy is 0.9182000160217285\n",
      "After 3701 training steps, loss on training batch is 0.4006459414958954,test accuracy is 0.9156000018119812\n",
      "After 3801 training steps, loss on training batch is 0.3535575866699219,test accuracy is 0.9157000184059143\n",
      "After 3901 training steps, loss on training batch is 0.28417855501174927,test accuracy is 0.91839998960495\n",
      "After 4001 training steps, loss on training batch is 0.25414130091667175,test accuracy is 0.9121999740600586\n",
      "After 4101 training steps, loss on training batch is 0.16910111904144287,test accuracy is 0.916700005531311\n",
      "After 4201 training steps, loss on training batch is 0.15889982879161835,test accuracy is 0.9211000204086304\n",
      "After 4301 training steps, loss on training batch is 0.4152103662490845,test accuracy is 0.9172000288963318\n",
      "After 4401 training steps, loss on training batch is 0.3014428913593292,test accuracy is 0.9129999876022339\n",
      "After 4501 training steps, loss on training batch is 0.24851273000240326,test accuracy is 0.9182999730110168\n",
      "After 4601 training steps, loss on training batch is 0.2348417341709137,test accuracy is 0.9168000221252441\n",
      "After 4701 training steps, loss on training batch is 0.15648649632930756,test accuracy is 0.9218999743461609\n",
      "After 4801 training steps, loss on training batch is 0.1634361892938614,test accuracy is 0.9218000173568726\n",
      "After 4901 training steps, loss on training batch is 0.36834099888801575,test accuracy is 0.9200999736785889\n",
      "After 5001 training steps, loss on training batch is 0.3033217191696167,test accuracy is 0.92330002784729\n",
      "After 5101 training steps, loss on training batch is 0.24132803082466125,test accuracy is 0.9207000136375427\n",
      "After 5201 training steps, loss on training batch is 0.2207958847284317,test accuracy is 0.9199000000953674\n",
      "After 5301 training steps, loss on training batch is 0.15931156277656555,test accuracy is 0.9233999848365784\n",
      "After 5401 training steps, loss on training batch is 0.1459442675113678,test accuracy is 0.925599992275238\n",
      "After 5501 training steps, loss on training batch is 0.3556642532348633,test accuracy is 0.9243999719619751\n",
      "After 5601 training steps, loss on training batch is 0.2904342710971832,test accuracy is 0.9221000075340271\n",
      "After 5701 training steps, loss on training batch is 0.20027555525302887,test accuracy is 0.9232000112533569\n",
      "After 5801 training steps, loss on training batch is 0.21596089005470276,test accuracy is 0.9225999712944031\n",
      "After 5901 training steps, loss on training batch is 0.1174805611371994,test accuracy is 0.9266999959945679\n"
     ]
    }
   ],
   "source": [
    "mnist = load_data(\"mnist.npz\")\n",
    "train(mnist)\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./log\",tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
